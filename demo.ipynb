{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a GUI demo built with [gradio](https://gradio.app/), identical to our [HuggingFace Demo ðŸ¤—](https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text).\n",
    "\n",
    "By running through this notebook, you can deploy the demo on your own machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from models.tag2text import ram, tag2text_caption\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# init image transforms\n",
    "image_size = 384\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load RAM Model\n",
    "model_ram = ram(\n",
    "    pretrained='pretrained/ram_swin_large_14m.pth',\n",
    "    image_size=image_size,\n",
    "    vit='swin_l'\n",
    ").eval().to(device)\n",
    "\n",
    "# load Tag2Text Model\n",
    "model_tag2text = tag2text_caption(\n",
    "    pretrained='pretrained/tag2text_swin_14m.pth',\n",
    "    image_size=image_size,\n",
    "    vit='swin_b',\n",
    "    threshold=0.68\n",
    ").eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_ram(img):\n",
    "    with torch.no_grad():\n",
    "        img = transform(img).unsqueeze(0).to(device)\n",
    "        tags, tags_chinese = model_ram.generate_tag(img)\n",
    "        return tags[0], tags_chinese[0]\n",
    "\n",
    "\n",
    "def inference_with_t2t(img, input_tags):\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    if not input_tags or input_tags.lower() == 'none':\n",
    "        input_tags = None\n",
    "    else:\n",
    "        input_tags = [input_tags.replace(',', ' | ')]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        caption, tag_predict = model_tag2text.generate(\n",
    "            img,\n",
    "            tag_input=input_tags,\n",
    "            max_length=50,\n",
    "            return_tag_predict=True\n",
    "        )\n",
    "        if input_tags is None:\n",
    "            output_tags = tag_predict\n",
    "        else:\n",
    "            # re-inference with tag_input=None to get model output tags\n",
    "            _, output_tags = model_tag2text.generate(\n",
    "                img,\n",
    "                tag_input=None,\n",
    "                max_length=50,\n",
    "                return_tag_predict=True\n",
    "            )\n",
    "\n",
    "    return output_tags[0], caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def build_gui():\n",
    "\n",
    "    description = \"\"\"\n",
    "        <center><strong><font size='10'>Recognize Anything Model</font></strong></center>\n",
    "        <br>\n",
    "        Welcome to the Recognize Anything Model (RAM) and Tag2Text Model demo! <br><br>\n",
    "        <li>\n",
    "            <b>Recognize Anything Model:</b> Upload your image to get the <b>English and Chinese outputs of the image tags</b>!\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>Tag2Text Model:</b> Upload your image to get the <b>tags</b> and <b>caption</b> of the image.\n",
    "            Optional: You can also input specified tags to get the corresponding caption.\n",
    "        </li>\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    article = \"\"\"\n",
    "        <p style='text-align: center'>\n",
    "            RAM and Tag2Text is training on open-source datasets, and we are persisting in refining and iterating upon it.<br/>\n",
    "            <a href='https://recognize-anything.github.io/' target='_blank'>Recognize Anything: A Strong Image Tagging Model</a>\n",
    "            |\n",
    "            <a href='https://https://tag2text.github.io/' target='_blank'>Tag2Text: Guiding Language-Image Model via Image Tagging</a>\n",
    "            |\n",
    "            <a href='https://github.com/xinyu1205/Tag2Text' target='_blank'>Github Repo</a>\n",
    "        </p>\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    with gr.Blocks(title=\"Recognize Anything Model\") as demo:\n",
    "        # components\n",
    "        gr.HTML(description)\n",
    "\n",
    "        with gr.Tab(label=\"Recognize Anything Model\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    ram_in_img = gr.Image(type=\"pil\")\n",
    "                    with gr.Row():\n",
    "                        ram_btn_run = gr.Button(value=\"Run\")\n",
    "                        ram_btn_clear = gr.Button(value=\"Clear\")\n",
    "                with gr.Column():\n",
    "                    ram_out_tag = gr.Textbox(label=\"Tags\")\n",
    "                    ram_out_biaoqian = gr.Textbox(label=\"æ ‡ç­¾\")\n",
    "            # change examples as you like\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"images/1641173_2291260800.jpg\"],\n",
    "                    [\"images/openset_example.jpg\"]\n",
    "                ],\n",
    "                fn=inference_with_ram,\n",
    "                inputs=[ram_in_img],\n",
    "                outputs=[ram_out_tag, ram_out_biaoqian],\n",
    "                cache_examples=False\n",
    "            )\n",
    "\n",
    "        with gr.Tab(label=\"Tag2Text Model\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    t2t_in_img = gr.Image(type=\"pil\")\n",
    "                    t2t_in_tag = gr.Textbox(\n",
    "                        label=\"User Specified Tags (Optional, separated by comma)\")\n",
    "                    with gr.Row():\n",
    "                        t2t_btn_run = gr.Button(value=\"Run\")\n",
    "                        t2t_btn_clear = gr.Button(value=\"Clear\")\n",
    "                with gr.Column():\n",
    "                    t2t_out_tag = gr.Textbox(label=\"Tags\")\n",
    "                    t2t_out_cap = gr.Textbox(label=\"Caption\")\n",
    "            # change examples as you like\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"images/1641173_2291260800.jpg\", \"\"],\n",
    "                    [\"images/openset_example.jpg\", \"\"]\n",
    "                ],\n",
    "                fn=inference_with_t2t,\n",
    "                inputs=[t2t_in_img, t2t_in_tag],\n",
    "                outputs=[t2t_out_tag, t2t_out_cap],\n",
    "                cache_examples=False\n",
    "            )\n",
    "\n",
    "        gr.HTML(article)\n",
    "\n",
    "        # events\n",
    "        # run inference\n",
    "        ram_btn_run.click(\n",
    "            fn=inference_with_ram,\n",
    "            inputs=[ram_in_img],\n",
    "            outputs=[ram_out_tag, ram_out_biaoqian]\n",
    "        )\n",
    "        t2t_btn_run.click(\n",
    "            fn=inference_with_t2t,\n",
    "            inputs=[t2t_in_img, t2t_in_tag],\n",
    "            outputs=[t2t_out_tag, t2t_out_cap]\n",
    "        )\n",
    "\n",
    "        # comment out when deployging on huggingface due to internet latency\n",
    "        # images of two image panels should keep the same\n",
    "        # and clear old outputs when image changes\n",
    "        # def sync_img(v):\n",
    "        #     return [gr.update(value=v)] + [gr.update(value=\"\")] * 4\n",
    "\n",
    "        # ram_in_img.upload(fn=sync_img, inputs=[ram_in_img], outputs=[\n",
    "        #     t2t_in_img, ram_out_tag, ram_out_biaoqian, t2t_out_tag, t2t_out_cap\n",
    "        # ])\n",
    "        # ram_in_img.clear(fn=sync_img, inputs=[ram_in_img], outputs=[\n",
    "        #     t2t_in_img, ram_out_tag, ram_out_biaoqian, t2t_out_tag, t2t_out_cap\n",
    "        # ])\n",
    "        # t2t_in_img.clear(fn=sync_img, inputs=[t2t_in_img], outputs=[\n",
    "        #     ram_in_img, ram_out_tag, ram_out_biaoqian, t2t_out_tag, t2t_out_cap\n",
    "        # ])\n",
    "        # t2t_in_img.upload(fn=sync_img, inputs=[t2t_in_img], outputs=[\n",
    "        #     ram_in_img, ram_out_tag, ram_out_biaoqian, t2t_out_tag, t2t_out_cap\n",
    "        # ])\n",
    "\n",
    "        # clear all\n",
    "        def clear_all():\n",
    "            return [gr.update(value=None)] * 2 + [gr.update(value=\"\")] * 5\n",
    "\n",
    "        ram_btn_clear.click(fn=clear_all, inputs=[], outputs=[\n",
    "            ram_in_img, t2t_in_img,\n",
    "            ram_out_tag, ram_out_biaoqian, t2t_in_tag, t2t_out_tag, t2t_out_cap\n",
    "        ])\n",
    "        t2t_btn_clear.click(fn=clear_all, inputs=[], outputs=[\n",
    "            ram_in_img, t2t_in_img,\n",
    "            ram_out_tag, ram_out_biaoqian, t2t_in_tag, t2t_out_tag, t2t_out_cap\n",
    "        ])\n",
    "\n",
    "    return demo\n",
    "\n",
    "demo = build_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(\n",
    "    server_name=\"127.0.0.1\",  # localhost. use \"0.0.0.0\" to open to LAN\n",
    "    share=False  # use True to acquire a temporary public domain for sharing\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "523f14362536113a9aaf8abdd469f5064f3978ec857b89ec73e5ec5e937174a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
